# Career Search Helper

## Purpose
This project aimed to create a program that would help people navigate the job search process, especially when they have a specific job and preference for a state where they would like to work in mind. This is done by scraping Indeed.com. Originally, I wanted to implement this function on BrownConnect, however, due to the need to login to that website, along with the two-factor authentication that is needed, I opted for a website that does not require the user to login to see the results. Additionally, our scraping program works with the first page of results that Indeed.com comes up with when given a specific prompt. This limitation is because further results require separate web pages, and although I was able to figure out how to scrape embedded links, I was unable to figure out how to scrape web pages in a for loop. 

### Design Choices:
#### Data Included in the Job Class:
After looking at Indeed, I noticed that the website listed the salary ranges for most jobs, the company name, location, how many days since the job had been posted, and job descriptions. However, the days since posted data was sometimes replaced with the number of days since the employer was active. The job title itself was a critical piece of data I decided to leave out. I did this because the driver-generator takes in user input, where the user can enter the job title they are looking for. Because the provided webpage is based on the given role, I thought it would be redundant also to list the job title in our data structure. 

#### Web Scraping
The central portion of this program is the web scraping function entitled scrape_data. This function can scrape web pages embedded in the original page, allowing us to scrape information from specific job pages rather than just limiting them to the data listed within the search result page. To make this scraper work efficiently, many helper functions were written to obtain and clean our scraped data so that the information could be inputted seamlessly into our other functions. After being scraped and cleaned, the types used for each data point within our Job class are as follows: average_salary: float, company(name): string, location: string, specifically state abbreviations, days_since_posted: integer, description: string. All the scraped Jobs were then put into a dictionary that used state abbreviations as the keys and whose values were lists of the Job Items in that state. 

#### Additional Functions
Lastly, additional functions were created to help the user discover different things about the scraped data. For example, if the user knew that they wanted to live in California, they could find out the number of opportunities in that state, which company most recently published an opening, and the name of the company that is paying the most for the given position. If the user did not know what state they wished to work in, they could find out the number of opportunities available in each state and the state that offers them the most opportunities. If there is something specific that the user wants in a jobâ€™s description, they can search all scraped job postings for a particular word. 

## Testing: 
For testing, the syntax for many of these functions already handles the various test cases that could be present by simply returning none for any case where the data is not given. Therefore, many of the test cases that I had to test for simply lacked data, which I did. For the helper functions above used to format data properly, these are implicitly tested through running the scraper, therefore not needing any external testing. To ensure that our function was correct, I created dictionaries whose values were lists that included Job listings and then used pytest syntax to ensure our function worked properly. 
